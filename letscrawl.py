import requests
from bs4 import BeautifulSoup
import json
import time
from typing import List, Dict, Optional
import math

class CompanyCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
            "Referer": "https://icis.me.go.kr/pageLink.do",
            "Origin": "https://icis.me.go.kr",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "X-Requested-With": "XMLHttpRequest"
        }
    
    def send_post_request(self, url: str, data: Dict) -> Dict:
        """POST ìš”ì²­ì„ ë³´ë‚´ê³  JSON ì‘ë‹µ ë°˜í™˜"""
        try:
            res = self.session.post(url, headers=self.headers, data=data)
            res.raise_for_status()
            return res.json()
        except Exception as e:
            print(f"âŒ POST ìš”ì²­ ì‹¤íŒ¨: {e}")
            return {}

    def fetch_company_page_by_material(
        self, material_name: str, year: str, page: int
    ) -> List[Dict]:
        """ì£¼ì–´ì§„ ë¬¼ì§ˆëª…ê³¼ ì—°ë„, í˜ì´ì§€ ë²ˆí˜¸ë¡œ ì—…ì²´ ëª©ë¡ 1í˜ì´ì§€ ì¡°íšŒ"""
        url = "https://icis.me.go.kr/iprtr/cdrInfoDetailListJson.do"
        payload = {
            "search1": year,
            "search3": "",
            "search4": "",
            "search5": "ì „ì²´ì§€ì—­",
            "search6": "",
            "search7": "ì „ì²´ì§€ì—­",
            "mttrGroup": "",
            "searchCategory": "03",
            "searchMttrWord": material_name,
            "irsttList": "",
            "level": "",
            "indutyCode": "",
            "indutyCode2": "",
            "indutyCode3": "",
            "indutyCode4": "",
            "pageNo": str(page),
        }

        response_json = self.send_post_request(url, payload)
        return response_json.get("list", [])

    def search_companies_by_material(
        self, material_name: str, year: str = "2022", max_companies: int = 10
    ) -> List[Dict]:
        """ë¬¼ì§ˆëª…ìœ¼ë¡œ ì—…ì²´ ê²€ìƒ‰ í›„ max_companies ê°œìˆ˜ë§Œí¼ ì—…ì²´ ëª©ë¡ ìˆ˜ì§‘"""
        print(f"ğŸš€ ë¬¼ì§ˆëª… '{material_name}' ê¸°ë°˜ ì—…ì²´ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        print(f"ğŸ“… ê²€ìƒ‰ë…„ë„: {year}, ìµœëŒ€ ì—…ì²´ìˆ˜: {max_companies}")
        print("-" * 60)

        companies = []
        page = 1

        while len(companies) < max_companies:
            print(f"ğŸ“„ í˜ì´ì§€ {page} ì¡°íšŒ ì¤‘...")
            page_companies = self.fetch_company_page_by_material(material_name, year, page)
            if not page_companies:
                print("âš ï¸ ë” ì´ìƒ ì¡°íšŒí•  ì—…ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break

            companies.extend(page_companies)
            if len(page_companies) == 0:
                break  # í˜ì´ì§€ì— ë” ì´ìƒ ë°ì´í„° ì—†ìŒ
            page += 1

        # max_companies ì´ìƒì´ë©´ ìë¦„
        companies = companies[:max_companies]

        print(f"âœ… ì´ ìˆ˜ì§‘ëœ ì—…ì²´ ìˆ˜: {len(companies)}")
        return companies

    
    def get_all_chemicals(self, bplc_id: str, year: str = "2022", page_unit: int = 200) -> List[Dict]:
        base_url = "https://icis.me.go.kr/iprtr/selectListOpenMatter.do"
        all_data = []
        page_no = 1
        
        while True:
            params = {
                "searchYear": year,
                "bplcId": bplc_id,
                "downPageNo": page_no,
                "downPageUnit": page_unit,
            }
            res = self.session.get(base_url, params=params, headers=self.headers)
            res.raise_for_status()
            json_data = res.json()
            items = json_data.get("result", [])
            
            if not items:
                break
            
            all_data.extend(items)
            
            # ì´ ê°œìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì¢…ë£Œ íŒë‹¨
            total = items[0].get("TOTALCOUNT", 0)
            total_pages = math.ceil(total / page_unit)
            if page_no >= total_pages:
                break
            page_no += 1
        
        return all_data
        
    
    def get_company_details(self, bplc_id: str, year: str = "2022") -> Dict:
        """ì—…ì²´ IDë¡œ ìƒì„¸ì •ë³´ ì¡°íšŒ (ìµœëŒ€ 3íšŒ ì¬ì‹œë„)"""
        detail_url = 'https://icis.me.go.kr/iprtr/cdrInfoView.do'
        detail_headers = {
            'Content-Type': 'application/x-www-form-urlencoded',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://icis.me.go.kr/pageLink.do'
        }
        
        detail_data = {
            'bplcId': bplc_id,
            'streNo': '',
            'searchYear': year,
        }

        max_retries = 3
        for attempt in range(1, max_retries + 1):
            try:
                print(f"ğŸ“‹ ì—…ì²´ ID '{bplc_id}' ìƒì„¸ì •ë³´ ì¡°íšŒ ì‹œë„ {attempt}íšŒì°¨...")
                response = self.session.post(detail_url, headers=detail_headers, data=detail_data)
                response.encoding = response.apparent_encoding
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ê¸°ë³¸ ì •ë³´ í…Œì´ë¸” ì°¾ê¸°
                basic_table = soup.find('table', class_='view_table')
                if not basic_table:
                    basic_table = soup.find('table', class_='viewTypeA')
                if not basic_table:
                    basic_table = soup.find('table', class_='tbl_st3')
                
                result = {"bplcId": bplc_id}
                
                if basic_table:
                    tbody = basic_table.find('tbody')
                    rows = tbody.find_all('tr') if tbody else basic_table.find_all('tr')
                    
                    def extract_field(field_name):
                        for row in rows:
                            th_list = row.find_all('th')
                            td_list = row.find_all('td')
                            for th, td in zip(th_list, td_list):
                                if field_name in th.text.strip():
                                    return td.text.strip()
                        return None
                    
                    fields = [
                        'ì—…ì²´ëª…', 'ëŒ€í‘œì', 'ì†Œì¬ì§€', 'ëŒ€í‘œì—…ì¢…',
                        'ì¢…ì—…ì›ìˆ˜', 'ê´€í•  í™˜ê²½ì²­', 'ì‚¬ì—…ì¥ ë¹„ìƒì—°ë½ë²ˆí˜¸',
                        'ì„¤ë¦½ë…„ë„', 'ìë³¸ê¸ˆ', 'ë§¤ì¶œì•¡', 'ì—…ì¢…ë¶„ë¥˜'
                    ]
                    
                    for field in fields:
                        value = extract_field(field)
                        if value:
                            result[field] = value
                
                if 'ì—…ì²´ëª…' not in result or not result['ì—…ì²´ëª…']:
                    company_name = soup.select_one("td.title strong")
                    if company_name:
                        result['ì—…ì²´ëª…'] = company_name.text.strip()
                
                chemical_data = self.get_all_chemicals(bplc_id, year)
                if chemical_data:
                    result['í™”í•™ë¬¼ì§ˆì •ë³´'] = chemical_data
                    print(f"âœ… ì—…ì²´ '{result.get('ì—…ì²´ëª…', bplc_id)}' ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ (í™”í•™ë¬¼ì§ˆ {len(chemical_data)}ê°œ)")
                else:
                    print(f"âœ… ì—…ì²´ '{result.get('ì—…ì²´ëª…', bplc_id)}' ê¸°ë³¸ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
                
                return result

            except Exception as e:
                print(f"âŒ ì—…ì²´ ID '{bplc_id}' ìƒì„¸ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ (ì‹œë„ {attempt}): {e}")
                if attempt < max_retries:
                    time.sleep(2)  # 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                else:
                    print(f"âŒ ìµœëŒ€ ì¬ì‹œë„ {max_retries}íšŒ ì‹¤íŒ¨, ì¡°íšŒ í¬ê¸°")
                    return {"bplcId": bplc_id, "error": str(e)}
            
    def crawl_companies_by_material(self, material_name: str, year: str = "2022", 
                                  max_companies: int = 10, delay: float = 1.0) -> List[Dict]:
        """ë¬¼ì§ˆëª…ìœ¼ë¡œ ì—…ì²´ ê²€ìƒ‰ í›„ ëª¨ë“  ì—…ì²´ì˜ ìƒì„¸ì •ë³´ ìˆ˜ì§‘"""
        print(f"ğŸš€ ë¬¼ì§ˆëª… '{material_name}' ê¸°ë°˜ ì—…ì²´ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        print(f"ğŸ“… ê²€ìƒ‰ë…„ë„: {year}, ìµœëŒ€ ì—…ì²´ìˆ˜: {max_companies}, ìš”ì²­ê°„ê²©: {delay}ì´ˆ")
        print("-" * 60)
        
        session = requests.Session()
        headers = {
            "User-Agent": "Mozilla/5.0",
            "Content-Type": "application/x-www-form-urlencoded",
        }
        
        # 1. ì—…ì²´ ëª©ë¡ ì¡°íšŒ
        companies = self.search_companies_by_material(material_name, year, max_companies)
        if not companies:
            return []
        
        # 2. ê° ì—…ì²´ ìƒì„¸ì •ë³´ ìˆ˜ì§‘
        all_details = []
        for i, company in enumerate(companies, 1):
            bplc_id = company.get("bplcId")
            if not bplc_id:
                continue
                
            print(f"[{i}/{len(companies)}] ", end="")
            detail_info = self.get_company_details(bplc_id, year)
            
            # ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ë„ í¬í•¨
            detail_info.update({
                "ê²€ìƒ‰_ìˆœë²ˆ": i,
                "ì›ë³¸_ì—…ì²´ëª…": company.get("bplcNm", ""),
                "ì›ë³¸_ì†Œì¬ì§€": company.get("addr", ""),
            })
            
            all_details.append(detail_info)
            
            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if i < len(companies):
                time.sleep(delay)
        
        print("-" * 60)
        print(f"ğŸ‰ ì´ {len(all_details)}ê°œ ì—…ì²´ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ!")
        return all_details
    
    def save_to_json(self, data: List[Dict], filename: str = None) -> str:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            filename = f"ì—…ì²´ì •ë³´_{int(time.time())}.json"
        
        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = CompanyCrawler()
    
    # ì‚¬ìš©ì ì…ë ¥
    material_name = input("ğŸ” ì¡°íšŒí•  ë¬¼ì§ˆëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 7727-21-1): ").strip()
    if not material_name:
        print("âŒ ë¬¼ì§ˆëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    year = input("ğŸ“… ê²€ìƒ‰í•  ì—°ë„ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 2022): ").strip() or "2022"
    
    try:
        max_companies = int(input("ğŸ“Š ìµœëŒ€ ì¡°íšŒí•  ì—…ì²´ ìˆ˜ (ê¸°ë³¸ê°’: 10): ").strip() or "10")
    except ValueError:
        max_companies = 10
    
    try:
        delay = float(input("â±ï¸  ìš”ì²­ ê°„ê²©(ì´ˆ, ê¸°ë³¸ê°’: 1.0): ").strip() or "1.0")
    except ValueError:
        delay = 1.0
    
    print("\n" + "="*60)
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    results = crawler.crawl_companies_by_material(
        material_name=material_name,
        year=year,
        max_companies=max_companies,
        delay=delay
    )
    
    if results:
        # ê²°ê³¼ ì €ì¥
        filename = f"{material_name}_{year}ë…„_ì—…ì²´ì •ë³´.json"
        crawler.save_to_json(results, filename)
        
        # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
        print("\nğŸ“ˆ ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½:")
        print(f"   - ì´ ì—…ì²´ ìˆ˜: {len(results)}")
        successful = len([r for r in results if 'error' not in r])
        print(f"   - ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ëœ ì—…ì²´: {successful}")
        if successful < len(results):
            print(f"   - ìˆ˜ì§‘ ì‹¤íŒ¨ ì—…ì²´: {len(results) - successful}")
        
        # í™”í•™ë¬¼ì§ˆ ì •ë³´ í†µê³„
        total_chemicals = sum(len(r.get('í™”í•™ë¬¼ì§ˆì •ë³´', [])) for r in results)
        companies_with_chemicals = len([r for r in results if r.get('í™”í•™ë¬¼ì§ˆì •ë³´')])
        if total_chemicals > 0:
            print(f"   - í™”í•™ë¬¼ì§ˆ ì •ë³´ê°€ ìˆëŠ” ì—…ì²´: {companies_with_chemicals}")
            print(f"   - ì´ í™”í•™ë¬¼ì§ˆ ë°ì´í„°: {total_chemicals}ê°œ")
        
        # ìƒìœ„ 3ê°œ ì—…ì²´ëª… ì¶œë ¥
        print("\nğŸ“‹ ìˆ˜ì§‘ëœ ì£¼ìš” ì—…ì²´:")
        for i, result in enumerate(results[:3], 1):
            company_name = result.get('ì—…ì²´ëª…', result.get('ì›ë³¸_ì—…ì²´ëª…', f"ì—…ì²´ID_{result.get('bplcId')}"))
            chemical_count = len(result.get('í™”í•™ë¬¼ì§ˆì •ë³´', []))
            chemical_info = f" (í™”í•™ë¬¼ì§ˆ {chemical_count}ê°œ)" if chemical_count > 0 else ""
            print(f"   {i}. {company_name}{chemical_info}")
        
        if len(results) > 3:
            print(f"   ... ì™¸ {len(results)-3}ê°œ ì—…ì²´")
    
    else:
        print("âŒ ìˆ˜ì§‘ëœ ì—…ì²´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
